{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bfa557f-bd90-44d8-bd85-fe540ce9ed85",
   "metadata": {},
   "source": [
    "# ESM Protein Folding using HuggingFace\n",
    "\n",
    "The [ESMFold protein language model](https://github.com/facebookresearch/esm) will fold proteins based only on the amino acid sequence. Accurate protein folding is critical to the design and optimization of new medicines. \n",
    "\n",
    "We'll use [HuggingFace esmfold_v1](https://huggingface.co/facebook/esmfold_v1) in this example, but there are many other protein folding models out there. This example also demonstrates how to handle multimer (proteins with multiple chains) predictions.\n",
    "\n",
    "This code ran on a Microsoft Surface Laptop with an NVIDIA RTX 2000 Ada using [WSL2 Ubuntu](https://learn.microsoft.com/en-us/windows/wsl/install) (8 GB GPU RAM, Driver Version: 537.58, CUDA Version: 12.2). It took ~ 11 minutes.\n",
    "\n",
    "```\n",
    "+---------------------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 535.161.07             Driver Version: 537.58       CUDA Version: 12.2     |\n",
    "|-----------------------------------------+----------------------+----------------------+\n",
    "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
    "|                                         |                      |               MIG M. |\n",
    "|=========================================+======================+======================|\n",
    "|   0  NVIDIA RTX 2000 Ada Gene...    On  | 00000000:01:00.0 Off |                  N/A |\n",
    "| N/A   54C    P1              30W /  60W |   7950MiB /  8188MiB |    100%      Default |\n",
    "|                                         |                      |                  N/A |\n",
    "+-----------------------------------------+----------------------+----------------------+\n",
    "\n",
    "+---------------------------------------------------------------------------------------+\n",
    "| Processes:                                                                            |\n",
    "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
    "|        ID   ID                                                             Usage      |\n",
    "|=======================================================================================|\n",
    "|    0   N/A  N/A     71450      C   /python3.12                               N/A      |\n",
    "+---------------------------------------------------------------------------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3769b1e8-0dfb-433b-b10d-e36d5c627181",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b51bf5c-1167-4fe6-9a5c-761e091593a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bed0326-6a36-4c09-9352-0d12f1a114b9",
   "metadata": {},
   "source": [
    "## Load HuggingFace model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e7bc44b-3124-4471-b816-ebc177f93a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, EsmForProteinFolding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9747493-1dc3-4084-b165-0fb835192e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"facebook/esmfold_v1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = EsmForProteinFolding.from_pretrained(model_name, low_cpu_mem_usage=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55372ba6-3c5f-436c-9452-a6d829461e69",
   "metadata": {},
   "source": [
    "Put tensor(s) on the desired hardware device. If CUDA (GPU) is available, then use that. If not, then use CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "333a6ac4-30a6-4af6-a1de-5aaafa34970a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# If you are on limited GPU resources (< 16 GB of GPU RAM), then use this.\n",
    "if torch.cuda.is_available():\n",
    "    model.esm = model.esm.half()\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    # Use chunks if your GPU memory is 16GB or less\n",
    "    model.trunk.set_chunk_size(64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799849ec-c50c-4e15-aed2-1654595a2763",
   "metadata": {},
   "source": [
    "## Multimers\n",
    "\n",
    "If the protein consists of multiple chains (multimers), then connect them as one long sequence string by inserting a chain of \"G\" in between.\n",
    "\n",
    "In this example, we'll use antibody [3HFM](https://www.rcsb.org/sequence/3HFM) which has heavy (H) and light (L) chains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "745a4147-cbfe-47cc-9eb9-0e6b1c40e3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_H = \"DVQLQESGPSLVKPSQTLSLTCSVTGDSITSDYWSWIRKFPGNRLEYMGYVSYSGSTYYNPSLKSRISITRDTSKNQYYLDLNSVTTEDTATYYCANWDGDYWGQGTLVTVSAAKTTPPSVYPLAPGSAAQTNSMVTLGCLVKGYF\"\n",
    "chain_L = \"DIVLTQSPATLSVTPGNSVSLSCRASQSIGNNLHWYQQKSHESPRLLIKYASQSISGIPSRFSGSGSGTDFTLSINSVETEDFGMYFCQQSNSWPYTFGGGTKLEIKRADAAPTVSIFPPSSEQLTSGGASVVCFLNNFYPKDINVKWKIDGSERQNGVLNSWTDQDSKDSTYSMSSTLTLTKDEYERHNSYTCEATHKTSTSPIVKSFNRNEC\"\n",
    "\n",
    "linker_sequence = \"G\" * 25  # Put G linker in between chains (hide it later)\n",
    "\n",
    "multimer_sequence = chain_H + linker_sequence + chain_L"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8cb2a6-f9dd-40b6-b096-3694fc283464",
   "metadata": {},
   "source": [
    "Tokenize the input sequence string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3cbd1e57-fa4f-4575-8cfb-958f7d5ca5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_multimer = tokenizer(\n",
    "    [multimer_sequence], return_tensors=\"pt\", add_special_tokens=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc31bad9-3490-4667-a661-5dc794dec0bd",
   "metadata": {},
   "source": [
    "Renumber the positions of the second chain so that the model knows that the second chain is not really connected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99c492d3-8867-45b1-ae16-9f454f600bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    position_ids = torch.arange(len(multimer_sequence), dtype=torch.long)\n",
    "    position_ids[len(chain_H) + len(linker_sequence) :] += 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1798bdba-ad3c-4f5b-a4b7-e00b43a4c5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_multimer[\"position_ids\"] = position_ids.unsqueeze(0)\n",
    "\n",
    "tokenized_multimer = {\n",
    "    key: tensor.to(device) for key, tensor in tokenized_multimer.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d323454e-4c9e-4c9a-8cbc-63ff9b398200",
   "metadata": {},
   "source": [
    "### Use the model to predict the 3D structure from the sequence input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff25c23d-92a7-49fa-a148-22c929b9bacf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model inference started at: 2024-04-02 17:45:58.391721\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "start_time = datetime.now()\n",
    "print(f\"Model inference started at: {start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7812743e-feef-4a00-90c5-9c5d0401e08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output = model(**tokenized_multimer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95b2c9e1-779b-471a-8c3c-447edaad6059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model inference stopped at: 2024-04-02 17:56:58.033758\n",
      "Elapsed time = 0:10:59.642037\n"
     ]
    }
   ],
   "source": [
    "stop_time = datetime.now()\n",
    "\n",
    "print(f\"Model inference stopped at: {stop_time}\")\n",
    "print(f\"Elapsed time = {stop_time - start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42af4730-e246-40f2-a5e3-c8ec49ce7764",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['frames', 'sidechain_frames', 'unnormalized_angles', 'angles', 'positions', 'states', 's_s', 's_z', 'distogram_logits', 'lm_logits', 'aatype', 'atom14_atom_exists', 'residx_atom14_to_atom37', 'residx_atom37_to_atom14', 'atom37_atom_exists', 'residue_index', 'lddt_head', 'plddt', 'ptm_logits', 'ptm', 'aligned_confidence_probs', 'predicted_aligned_error', 'max_predicted_aligned_error'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74c8be5-96f6-494c-857e-f9a197c253c7",
   "metadata": {},
   "source": [
    "## Mask the linker section in the output\n",
    "\n",
    "This will mask the linker section so that when the PDB file is created from the output tensor, then it will hide the GGGGG linker that we put in to separate the chains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "937a6882-e624-44a6-8a3e-78aa56c813dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "linker_mask = torch.tensor(\n",
    "    [1] * len(chain_H) + [0] * len(linker_sequence) + [1] * len(chain_L)\n",
    ")[None, :, None]\n",
    "\n",
    "output[\"atom37_atom_exists\"] = output[\"atom37_atom_exists\"] * linker_mask.to(\n",
    "    output[\"atom37_atom_exists\"].device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a67f7f-56b7-4cc9-8462-0a0135052858",
   "metadata": {},
   "source": [
    "### Convert ESMFold output tensor to a PDB file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d7a64076-643f-4823-92a7-316b65764b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.esm.openfold_utils.feats import atom14_to_atom37\n",
    "from transformers.models.esm.openfold_utils.protein import Protein as OFProtein\n",
    "from transformers.models.esm.openfold_utils.protein import to_pdb\n",
    "\n",
    "\n",
    "def convert_outputs_to_pdb(outputs):\n",
    "    \"\"\"Convert from the ESMFold model output to a PDB-formatted protein string\n",
    "\n",
    "    Args:\n",
    "        outputs: Output tensor of HuggingFace ESMFold model\n",
    "\n",
    "    Returns:\n",
    "        String with formatted PDB of protein structure\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    final_atom_positions = atom14_to_atom37(outputs[\"positions\"][-1], outputs)\n",
    "    outputs_dictionary = {key: value.to(\"cpu\").numpy() for key, value in outputs.items()}\n",
    "    final_atom_positions = final_atom_positions.cpu().numpy()\n",
    "    final_atom_mask = outputs_dictionary[\"atom37_atom_exists\"]\n",
    "    \n",
    "    pdbs = []\n",
    "    \n",
    "    for idx in range(outputs_dictionary[\"aatype\"].shape[0]):\n",
    "        aa = outputs_dictionary[\"aatype\"][idx]\n",
    "        predicted_positions = final_atom_positions[idx]\n",
    "        mask = final_atom_mask[idx]\n",
    "        residues = outputs_dictionary[\"residue_index\"][idx] + 1\n",
    "        pred = OFProtein(\n",
    "            aatype=aa,\n",
    "            atom_positions=predicted_positions,\n",
    "            atom_mask=mask,\n",
    "            residue_index=residues,\n",
    "            b_factors=outputs_dictionary[\"plddt\"][i],\n",
    "            chain_index=outputs_dictionary[\"chain_index\"][idx] if \"chain_index\" in outputs else None,\n",
    "        )\n",
    "        pdbs.append(to_pdb(pred))\n",
    "\n",
    "    return pdbs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491588f0-29c2-4549-b6e7-3e746fc8bec4",
   "metadata": {},
   "source": [
    "### Convert the model output\n",
    "\n",
    "Convert the model output to a PDB structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e383192-026e-4057-8b07-bb352cbdeb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdb = convert_outputs_to_pdb(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed79c65-66ae-44e3-a44b-be638531d496",
   "metadata": {},
   "source": [
    "### Display the protein folding prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ea7de3-e7e7-4e7a-8cb8-9d0f5ccac753",
   "metadata": {},
   "outputs": [],
   "source": [
    "import py3Dmol\n",
    "\n",
    "view = py3Dmol.view(js=\"https://3dmol.org/build/3Dmol.js\", width=800, height=400)\n",
    "view.addModel(\"\".join(pdb), \"pdb\")\n",
    "view.setStyle({\"model\": -1}, {\"cartoon\": {\"color\": \"spectrum\"}})\n",
    "view.zoomTo()\n",
    "view.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07113ba0-8202-46df-bfc6-5f3cbe22f613",
   "metadata": {},
   "source": [
    "## Write the PDB to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4911d373-c757-462b-ba21-d1a8d252eb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_pdb_filename = \"prediction_3HFM.pdb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5795dc66-da3a-4390-937c-127420e3ae95",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(prediction_pdb_filename, \"w\") as fptr:\n",
    "    fptr.write(\"HEADER    COMPLEX(ANTIBODY-ANTIGEN)   3HFM ESMFold prediction\\n\")\n",
    "    fptr.write(pdb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6de0fe9-c58f-4b90-903d-ddd6161bf70c",
   "metadata": {},
   "source": [
    "You can compare the prediction with the real structure by using [USAlign](https://zhanggroup.org/US-align/).\n",
    "\n",
    "The real 3HFM PDB structure is on [RCSB](https://files.rcsb.org/download/3HFM.pdb). Note that this structure also includes the antigen. So only two of the 3 chains will align (but USalign will compensate for that).\n",
    "\n",
    "> Aligned length= 214, RMSD=   1.35, Seq_ID=n_identical/n_aligned= 1.000\n",
    "TM-score= 0.94787 (normalized by length of Structure_1: L=214, d0=5.44)\n",
    "> The root mean squared error (distance) between the predicted structure and the measured structure is 1.35 Angstrom. For comparison, the diameter of a single water molecule is about 2.75 Angstrom. The template matching score is 0.948 (perfect match = 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae7141d-77d0-4b93-92c3-88837d2f894e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh \n",
    "wget --no-clobber https://zhanggroup.org/US-align/bin/module/USalign.cpp\n",
    "wget --no-clobber https://files.rcsb.org/download/3HFM.pdb\n",
    "if [ ! -f USalign ]; then\n",
    "    g++ -v -static -O3 -ffast-math -o USalign USalign.cpp\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "80f2f8fa-85bc-4e95-a44a-86992f7e5f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ********************************************************************\n",
      " * US-align (Version 20240319)                                      *\n",
      " * Universal Structure Alignment of Proteins and Nucleic Acids      *\n",
      " * Reference: C Zhang, M Shine, AM Pyle, Y Zhang. (2022) Nat Methods*\n",
      " *            C Zhang, AM Pyle (2022) iScience.                     *\n",
      " * Please email comments and suggestions to zhang@zhanggroup.org    *\n",
      " ********************************************************************\n",
      "\n",
      "Name of Structure_1: 3HFM.pdb:L (to be superimposed onto Structure_2)\n",
      "Name of Structure_2: prediction_3HFM.pdb:A\n",
      "Length of Structure_1: 214 residues\n",
      "Length of Structure_2: 360 residues\n",
      "\n",
      "Aligned length= 214, RMSD=   1.35, Seq_ID=n_identical/n_aligned= 1.000\n",
      "TM-score= 0.94787 (normalized by length of Structure_1: L=214, d0=5.44)\n",
      "TM-score= 0.57416 (normalized by length of Structure_2: L=360, d0=6.90)\n",
      "(You should use TM-score normalized by length of the reference structure)\n",
      "\n",
      "(\":\" denotes residue pairs of d < 5.0 Angstrom, \".\" denotes other aligned residues)\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------DIVLTQSPATLSVTPGNSVSLSCRASQSIGNNLHWYQQKSHESPRLLIKYASQSISGIPSRFSGSGSGTDFTLSINSVETEDFGMYFCQQSNSWPYTFGGGTKLEIKRADAAPTVSIFPPSSEQLTSGGASVVCFLNNFYPKDINVKWKIDGSERQNGVLNSWTDQDSKDSTYSMSSTLTLTKDEYERHNSYTCEATHKTSTSPIVKSFNRNEC\n",
      "                                                                                                                                                  ::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::..\n",
      "DVQLQESGPSLVKPSQTLSLTCSVTGDSITSDYWSWIRKFPGNRLEYMGYVSYSGSTYYNPSLKSRISITRDTSKNQYYLDLNSVTTEDTATYYCANWDGDYWGQGTLVTVSAAKTTPPSVYPLAPGSAAQTNSMVTLGCLVKGYFDIVLTQSPATLSVTPGNSVSLSCRASQSIGNNLHWYQQKSHESPRLLIKYASQSISGIPSRFSGSGSGTDFTLSINSVETEDFGMYFCQQSNSWPYTFGGGTKLEIKRADAAPTVSIFPPSSEQLTSGGASVVCFLNNFYPKDINVKWKIDGSERQNGVLNSWTDQDSKDSTYSMSSTLTLTKDEYERHNSYTCEATHKTSTSPIVKSFNRNEC\n",
      "\n",
      "#Total CPU time is  0.07 seconds\n"
     ]
    }
   ],
   "source": [
    "!./USalign 3HFM.pdb $prediction_pdb_filename -o superimposed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b602d9-6f69-44f3-95b1-231641505dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "view = py3Dmol.view(js=\"https://3dmol.org/build/3Dmol.js\", width=800, height=400)\n",
    "view.addModel(open(\"superimposed.pdb\").read(), \"pdb\")\n",
    "view.setStyle({\"model\": -1}, {\"cartoon\": {\"color\": \"spectrum\"}})\n",
    "view.zoomTo()\n",
    "view.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
