{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bfa557f-bd90-44d8-bd85-fe540ce9ed85",
   "metadata": {},
   "source": [
    "# ESM Protein Folding using HuggingFace\n",
    "\n",
    "[ESMFold protein language model](https://github.com/facebookresearch/esm) to fold proteins based only on the protein sequence. \n",
    "\n",
    "This example also demonstrates how to handle multimer (proteins with multiple chains) predictions.\n",
    "\n",
    "This code ran on a Microsoft Surface Laptop with an NVIDIA RTX 2000 Ada using [WSL2 Ubuntu](https://learn.microsoft.com/en-us/windows/wsl/install) (8 GB GPU RAM, Driver Version: 537.58, CUDA Version: 12.2).\n",
    "\n",
    "```\n",
    "+---------------------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 535.161.07             Driver Version: 537.58       CUDA Version: 12.2     |\n",
    "|-----------------------------------------+----------------------+----------------------+\n",
    "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
    "|                                         |                      |               MIG M. |\n",
    "|=========================================+======================+======================|\n",
    "|   0  NVIDIA RTX 2000 Ada Gene...    On  | 00000000:01:00.0 Off |                  N/A |\n",
    "| N/A   54C    P1              30W /  60W |   7950MiB /  8188MiB |    100%      Default |\n",
    "|                                         |                      |                  N/A |\n",
    "+-----------------------------------------+----------------------+----------------------+\n",
    "\n",
    "+---------------------------------------------------------------------------------------+\n",
    "| Processes:                                                                            |\n",
    "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
    "|        ID   ID                                                             Usage      |\n",
    "|=======================================================================================|\n",
    "|    0   N/A  N/A     71450      C   /python3.12                               N/A      |\n",
    "+---------------------------------------------------------------------------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3769b1e8-0dfb-433b-b10d-e36d5c627181",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b51bf5c-1167-4fe6-9a5c-761e091593a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bed0326-6a36-4c09-9352-0d12f1a114b9",
   "metadata": {},
   "source": [
    "## Load HuggingFace model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7bc44b-3124-4471-b816-ebc177f93a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, EsmForProteinFolding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9747493-1dc3-4084-b165-0fb835192e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"facebook/esmfold_v1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = EsmForProteinFolding.from_pretrained(model_name, low_cpu_mem_usage=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55372ba6-3c5f-436c-9452-a6d829461e69",
   "metadata": {},
   "source": [
    "Put tensor(s) on the desired hardware device. If CUDA (GPU) is available, then use that. If not, then use CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333a6ac4-30a6-4af6-a1de-5aaafa34970a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model.esm = model.esm.half()\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    # Use chunks if your GPU memory is 16GB or less\n",
    "    model.trunk.set_chunk_size(64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799849ec-c50c-4e15-aed2-1654595a2763",
   "metadata": {},
   "source": [
    "## Multimers\n",
    "\n",
    "If the protein consists of multiple chains (multimers), then connect them as one long sequence string by inserting a chain of \"G\" in between.\n",
    "\n",
    "In this example, we'll use antibody [3HFM](https://www.rcsb.org/sequence/3HFM) which has heavy (H) and light (L) chains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745a4147-cbfe-47cc-9eb9-0e6b1c40e3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_H = \"DVQLQESGPSLVKPSQTLSLTCSVTGDSITSDYWSWIRKFPGNRLEYMGYVSYSGSTYYNPSLKSRISITRDTSKNQYYLDLNSVTTEDTATYYCANWDGDYWGQGTLVTVSAAKTTPPSVYPLAPGSAAQTNSMVTLGCLVKGYF\"\n",
    "chain_L = \"DIVLTQSPATLSVTPGNSVSLSCRASQSIGNNLHWYQQKSHESPRLLIKYASQSISGIPSRFSGSGSGTDFTLSINSVETEDFGMYFCQQSNSWPYTFGGGTKLEIKRADAAPTVSIFPPSSEQLTSGGASVVCFLNNFYPKDINVKWKIDGSERQNGVLNSWTDQDSKDSTYSMSSTLTLTKDEYERHNSYTCEATHKTSTSPIVKSFNRNEC\"\n",
    "\n",
    "linker_sequence = \"G\" * 25  # Put G linker in between chains (hide it later)\n",
    "\n",
    "multimer_sequence = chain_H + linker_sequence + chain_L"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8cb2a6-f9dd-40b6-b096-3694fc283464",
   "metadata": {},
   "source": [
    "Tokenize the input sequence string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbd1e57-fa4f-4575-8cfb-958f7d5ca5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_multimer = tokenizer([multimer_sequence], return_tensors=\"pt\", add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc31bad9-3490-4667-a661-5dc794dec0bd",
   "metadata": {},
   "source": [
    "Renumber the positions of the second chain so that the model knows that the second chain is not really connected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c492d3-8867-45b1-ae16-9f454f600bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    position_ids = torch.arange(len(multimer_sequence), dtype=torch.long)\n",
    "    position_ids[len(chain_H) + len(linker_sequence):] += 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1798bdba-ad3c-4f5b-a4b7-e00b43a4c5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_multimer[\"position_ids\"] = position_ids.unsqueeze(0)\n",
    "\n",
    "tokenized_multimer = {key: tensor.to(device) for key, tensor in tokenized_multimer.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d323454e-4c9e-4c9a-8cbc-63ff9b398200",
   "metadata": {},
   "source": [
    "### Use the model to predict the 3D structure from the sequence input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7812743e-feef-4a00-90c5-9c5d0401e08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output = model(**tokenized_multimer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42af4730-e246-40f2-a5e3-c8ec49ce7764",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74c8be5-96f6-494c-857e-f9a197c253c7",
   "metadata": {},
   "source": [
    "## Mask the linker section in the output\n",
    "\n",
    "This will mask the linker section so that when the PDB file is created from the output tensor, then it will hide the GGGGG linker that we put in to separate the chains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937a6882-e624-44a6-8a3e-78aa56c813dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "linker_mask = torch.tensor([1] * len(chain_H) + [0] * len(linker_sequence) + [1] * len(chain_L))[None, :, None]\n",
    "\n",
    "output[\"atom37_atom_exists\"] = output[\"atom37_atom_exists\"] * linker_mask.to(output[\"atom37_atom_exists\"].device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a67f7f-56b7-4cc9-8462-0a0135052858",
   "metadata": {},
   "source": [
    "### Convert ESMFold output tensor to a PDB file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7a64076-643f-4823-92a7-316b65764b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.esm.openfold_utils.protein import to_pdb, Protein as OFProtein\n",
    "from transformers.models.esm.openfold_utils.feats import atom14_to_atom37\n",
    "\n",
    "def convert_outputs_to_pdb(outputs):\n",
    "    \"\"\"Convert from the ESMFold model output to a PDB-formatted protein string\n",
    "\n",
    "    Args:\n",
    "        outputs: Output tensor of HuggingFace ESMFold model\n",
    "\n",
    "    Returns:\n",
    "        String with formatted PDB of protein structure\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    final_atom_positions = atom14_to_atom37(outputs[\"positions\"][-1], outputs)\n",
    "    outputs = {k: v.to(\"cpu\").numpy() for k, v in outputs.items()}\n",
    "    final_atom_positions = final_atom_positions.cpu().numpy()\n",
    "    final_atom_mask = outputs[\"atom37_atom_exists\"]\n",
    "    pdbs = []\n",
    "    for i in range(outputs[\"aatype\"].shape[0]):\n",
    "        aa = outputs[\"aatype\"][i]\n",
    "        pred_pos = final_atom_positions[i]\n",
    "        mask = final_atom_mask[i]\n",
    "        resid = outputs[\"residue_index\"][i] + 1\n",
    "        pred = OFProtein(\n",
    "            aatype=aa,\n",
    "            atom_positions=pred_pos,\n",
    "            atom_mask=mask,\n",
    "            residue_index=resid,\n",
    "            b_factors=outputs[\"plddt\"][i],\n",
    "            chain_index=outputs[\"chain_index\"][i] if \"chain_index\" in outputs else None,\n",
    "        )\n",
    "        pdbs.append(to_pdb(pred))\n",
    "        \n",
    "    return pdbs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491588f0-29c2-4549-b6e7-3e746fc8bec4",
   "metadata": {},
   "source": [
    "### Convert the model output\n",
    "\n",
    "Convert the model output to a PDB structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e383192-026e-4057-8b07-bb352cbdeb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdb = convert_outputs_to_pdb(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed79c65-66ae-44e3-a44b-be638531d496",
   "metadata": {},
   "source": [
    "### Display the protein folding prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ea7de3-e7e7-4e7a-8cb8-9d0f5ccac753",
   "metadata": {},
   "outputs": [],
   "source": [
    "import py3Dmol\n",
    "\n",
    "view = py3Dmol.view(js='https://3dmol.org/build/3Dmol.js', width=800, height=400)\n",
    "view.addModel(\"\".join(pdb), 'pdb')\n",
    "view.setStyle({'model': -1}, {\"cartoon\": {'color': 'spectrum'}})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07113ba0-8202-46df-bfc6-5f3cbe22f613",
   "metadata": {},
   "source": [
    "## Write the PDB to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce1f627-5730-4e25-855f-f1a67fc1e82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"3HFM_prediction.pdb\", \"w\") as fptr:\n",
    "    fptr.write(\"HEADER    COMPLEX(ANTIBODY-ANTIGEN)   3HFM ESMFold prediction\\n\")\n",
    "    fptr.write(pdb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6de0fe9-c58f-4b90-903d-ddd6161bf70c",
   "metadata": {},
   "source": [
    "You can compare the prediction with the real structure by using [USAlign](https://zhanggroup.org/US-align/).\n",
    "\n",
    "The real 3HFM PDB structure is on [RCSB](https://files.rcsb.org/download/3HFM.pdb). Note that this structure also includes the antigen. So only two of the 3 chains will align (but USalign will compensate for that).\n",
    "\n",
    "> Aligned length= 214, RMSD=   1.35, Seq_ID=n_identical/n_aligned= 1.000\n",
    "TM-score= 0.94787 (normalized by length of Structure_1: L=214, d0=5.44)\n",
    "> The root mean squared error (distance) between the predicted structure and the measured structure is 1.35 Angstrom. For comparison, the diameter of a single water molecule is about 2.75 Angstrom. The template matching score is 0.948 (perfect match = 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae7141d-77d0-4b93-92c3-88837d2f894e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
